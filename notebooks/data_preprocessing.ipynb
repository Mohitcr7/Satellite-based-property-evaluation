{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a **complete data preprocessing pipeline** for the property valuation project.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. **Load Data** - Read from Excel/CSV\n",
    "2. **Clean Data** - Remove outliers and invalid records\n",
    "3. **Engineer Features** - Create derived features\n",
    "4. **Scale Features** - Standardize numeric features\n",
    "5. **Save Data** - Export processed data\n",
    "\n",
    "**Key Principle:** Good preprocessing = better models. Garbage in â†’ garbage out.\n",
    "\n",
    "This notebook mirrors the production Python pipeline (`Data_processing_pipeline.py`) for transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STAGE 1: Load Data\n",
    "\n",
    "### Why This Matters:\n",
    "- **File Format Detection** - Automatically detect Excel vs CSV\n",
    "- **Error Handling** - Catch file not found or corruption issues\n",
    "- **Initial Profiling** - Document raw data shape and columns\n",
    "\n",
    "### Python Code Reference:\n",
    "```python\n",
    "def load_data(self):\n",
    "    if self.input_file.endswith('.xlsx'):\n",
    "        self.data = pd.read_excel(self.input_file)\n",
    "    elif self.input_file.endswith('.csv'):\n",
    "        self.data = pd.read_csv(self.input_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = '../data/raw/train.xlsx'  # Change path as needed\n",
    "\n",
    "try:\n",
    "    if file_path.endswith('.xlsx'):\n",
    "        df = pd.read_excel(file_path)\n",
    "    elif file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"File must be .xlsx or .csv\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"[1/5] DATA LOADING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data profiling\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STAGE 2: Clean Data\n",
    "\n",
    "### What is Data Cleaning?\n",
    "Removing records that are:\n",
    "- **Outliers** - Extreme values (e.g., mansion-sized house for $50K)\n",
    "- **Impossible** - Negative prices, 0 bedrooms\n",
    "- **Inconsistent** - Data entry errors\n",
    "\n",
    "### Our Thresholds:\n",
    "| Column | Min | Max | Reason |\n",
    "|--------|-----|-----|--------|\n",
    "| **sqft_lot** | - | 213,000 | Removes extraordinarily large lots |\n",
    "| **sqft_living** | 500 | - | Removes tiny houses (data errors) |\n",
    "| **bedrooms** | 1 | - | Removes 0-bedroom properties |\n",
    "| **bathrooms** | 0.5 | - | Allows 0.5-bath (studio), removes 0 |\n",
    "| **price** | 100,000 | - | Removes properties under $100K (errors) |\n",
    "\n",
    "### Python Code Reference:\n",
    "```python\n",
    "def clean_data(self):\n",
    "    if 'sqft_lot' in self.data.columns:\n",
    "        self.data = self.data[self.data['sqft_lot'] <= 213000]\n",
    "    if 'price' in self.data.columns:\n",
    "        self.data = self.data[self.data['price'] >= 100000]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "original_count = len(df_clean)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"[2/5] DATA CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nApplying quality thresholds...\")\n",
    "\n",
    "# Threshold 1: sqft_lot\n",
    "if 'sqft_lot' in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['sqft_lot'] <= 213000]\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"  â€¢ Removed {removed} properties with sqft_lot > 213,000\")\n",
    "\n",
    "# Threshold 2: sqft_living\n",
    "if 'sqft_living' in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['sqft_living'] >= 500]\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"  â€¢ Removed {removed} properties with sqft_living < 500\")\n",
    "\n",
    "# Threshold 3: bedrooms\n",
    "if 'bedrooms' in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['bedrooms'] >= 1]\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"  â€¢ Removed {removed} properties with bedrooms < 1\")\n",
    "\n",
    "# Threshold 4: bathrooms\n",
    "if 'bathrooms' in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['bathrooms'] >= 0.5]\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"  â€¢ Removed {removed} properties with bathrooms < 0.5\")\n",
    "\n",
    "# Threshold 5: price\n",
    "if 'price' in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['price'] >= 100000]\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"  â€¢ Removed {removed} properties with price < $100,000\")\n",
    "\n",
    "total_removed = original_count - len(df_clean)\n",
    "removal_pct = (total_removed / original_count) * 100\n",
    "\n",
    "print(f\"\\nCleaning Summary:\")\n",
    "print(f\"   Original records: {original_count:,}\")\n",
    "print(f\"   After cleaning: {len(df_clean):,}\")\n",
    "print(f\"   Removed: {total_removed:,} ({removal_pct:.2f}%)\")\n",
    "print(f\"   Retention rate: {(1-removal_pct/100)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact of cleaning\n",
    "if 'price' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Before cleaning\n",
    "    axes[0].hist(df['price'], bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Before Cleaning (Outliers Present)')\n",
    "    axes[0].set_xlabel('Price ($)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].axvline(df['price'].mean(), color='darkred', linestyle='--', linewidth=2, label=f'Mean: ${df[\"price\"].mean():,.0f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # After cleaning\n",
    "    axes[1].hist(df_clean['price'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('After Cleaning (Outliers Removed)')\n",
    "    axes[1].set_xlabel('Price ($)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].axvline(df_clean['price'].mean(), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: ${df_clean[\"price\"].mean():,.0f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STAGE 3: Engineer Features\n",
    "\n",
    "### What is Feature Engineering?\n",
    "Creating **new columns** from existing data that:\n",
    "- Capture domain knowledge (e.g., property_age from yr_built)\n",
    "- Normalize skewed distributions (e.g., log transforms)\n",
    "- Create meaningful interactions (e.g., grade Ã— condition)\n",
    "- Improve model interpretability\n",
    "\n",
    "### Features Created (10+ new features):\n",
    "\n",
    "| Feature | Type | Formula | Purpose |\n",
    "|---------|------|---------|----------|\n",
    "| **year, month, season** | Temporal | From date column | Capture seasonality |\n",
    "| **was_renovated** | Binary | yr_renovated > 0 | Renovation status |\n",
    "| **years_since_renovation** | Numeric | year - yr_renovated | Renovation recency |\n",
    "| **property_age** | Numeric | year - yr_built | Building age |\n",
    "| **total_rooms** | Numeric | bedrooms + bathrooms | Room count |\n",
    "| **sqft_per_lot** | Ratio | sqft_living / sqft_lot | Space utilization |\n",
    "| **above_neighborhood** | Binary | sqft_living > sqft_living15 | Relative size |\n",
    "| **quality_score** | Interaction | grade Ã— condition | Quality index |\n",
    "| **quality_grade_condition** | Additive | grade + condition | Alt. quality |\n",
    "| **price_log, sqft_living_log** | Log transform | log1p(feature) | Normalize skew |\n",
    "| **bedrooms_per_bathroom** | Ratio | bedrooms / bathrooms | Layout ratio |\n",
    "| **days_on_market** | Time | Days listing â†’ sale | Market velocity |\n",
    "\n",
    "### Python Code Reference:\n",
    "```python\n",
    "def engineer_features(self):\n",
    "    # Temporal features\n",
    "    self.data['year'] = self.data['date'].dt.year\n",
    "    self.data['month'] = self.data['date'].dt.month\n",
    "    \n",
    "    # Quality interaction\n",
    "    self.data['quality_score'] = self.data['grade'] * self.data['condition']\n",
    "    \n",
    "    # Log transform for skew\n",
    "    self.data['price_log'] = np.log1p(self.data['price'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_clean.copy()\n",
    "features_added = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"[3/5] FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nCreating new features...\")\n",
    "\n",
    "# ===== TEMPORAL FEATURES =====\n",
    "if 'date' in df_eng.columns:\n",
    "    df_eng['date'] = pd.to_datetime(df_eng['date'], format='%Y%m%dT%H%M%S')\n",
    "    df_eng['year'] = df_eng['date'].dt.year\n",
    "    df_eng['month'] = df_eng['date'].dt.month\n",
    "    \n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 0  # Winter\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 1  # Spring\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 2  # Summer\n",
    "        else:\n",
    "            return 3  # Fall\n",
    "    \n",
    "    df_eng['season'] = df_eng['month'].apply(get_season)\n",
    "    df_eng = df_eng.drop('date', axis=1)\n",
    "    features_added.extend(['year', 'month', 'season'])\n",
    "    print(\" Temporal: year, month, season\")\n",
    "\n",
    "# ===== RENOVATION FEATURES =====\n",
    "if 'yr_renovated' in df_eng.columns:\n",
    "    df_eng['was_renovated'] = (df_eng['yr_renovated'] > 0).astype(int)\n",
    "    features_added.append('was_renovated')\n",
    "    print(\" Renovation: was_renovated\")\n",
    "    \n",
    "    if 'year' in df_eng.columns:\n",
    "        df_eng['years_since_renovation'] = df_eng['year'] - df_eng['yr_renovated']\n",
    "        df_eng.loc[df_eng['yr_renovated'] == 0, 'years_since_renovation'] = 0\n",
    "        features_added.append('years_since_renovation')\n",
    "        print(\" Renovation: years_since_renovation\")\n",
    "\n",
    "# ===== PROPERTY AGE =====\n",
    "if 'yr_built' in df_eng.columns and 'year' in df_eng.columns:\n",
    "    df_eng['property_age'] = df_eng['year'] - df_eng['yr_built']\n",
    "    features_added.append('property_age')\n",
    "    print(\"Age: property_age\")\n",
    "\n",
    "# ===== ROOM RATIOS =====\n",
    "if 'bedrooms' in df_eng.columns and 'bathrooms' in df_eng.columns:\n",
    "    df_eng['total_rooms'] = df_eng['bedrooms'] + df_eng['bathrooms']\n",
    "    df_eng['bedrooms_per_bathroom'] = df_eng['bedrooms'] / (df_eng['bathrooms'] + 0.01)\n",
    "    features_added.extend(['total_rooms', 'bedrooms_per_bathroom'])\n",
    "    print(\" Ratios: total_rooms, bedrooms_per_bathroom\")\n",
    "\n",
    "# ===== SPACE RATIOS =====\n",
    "if 'sqft_living' in df_eng.columns and 'sqft_lot' in df_eng.columns:\n",
    "    df_eng['sqft_per_lot'] = df_eng['sqft_living'] / (df_eng['sqft_lot'] + 1)\n",
    "    features_added.append('sqft_per_lot')\n",
    "    print(\" Space: sqft_per_lot\")\n",
    "    \n",
    "    if 'sqft_living15' in df_eng.columns:\n",
    "        df_eng['above_neighborhood'] = (df_eng['sqft_living'] > df_eng['sqft_living15']).astype(int)\n",
    "        features_added.append('above_neighborhood')\n",
    "        print(\" Neighborhood: above_neighborhood\")\n",
    "\n",
    "# ===== QUALITY SCORES =====\n",
    "if 'grade' in df_eng.columns and 'condition' in df_eng.columns:\n",
    "    df_eng['quality_score'] = df_eng['grade'] * df_eng['condition']\n",
    "    df_eng['quality_grade_condition'] = df_eng['grade'] + df_eng['condition']\n",
    "    features_added.extend(['quality_score', 'quality_grade_condition'])\n",
    "    print(\" Quality: quality_score, quality_grade_condition\")\n",
    "\n",
    "# ===== LOG TRANSFORMS =====\n",
    "log_transforms = ['price', 'sqft_living', 'sqft_lot']\n",
    "for feature in log_transforms:\n",
    "    if feature in df_eng.columns:\n",
    "        df_eng[f'{feature}_log'] = np.log1p(df_eng[feature])\n",
    "        features_added.append(f'{feature}_log')\n",
    "print(f\"Log transforms: price_log, sqft_living_log, sqft_lot_log\")\n",
    "\n",
    "# ===== MARKET FEATURES =====\n",
    "if 'year' in df_eng.columns and 'month' in df_eng.columns:\n",
    "    df_eng['days_on_market'] = 30  # Default estimate\n",
    "    features_added.append('days_on_market')\n",
    "    print(\" Market: days_on_market\")\n",
    "\n",
    "print(f\"\\nFeature Engineering Summary:\")\n",
    "print(f\"Total new features created: {len(features_added)}\")\n",
    "print(f\" Features: {', '.join(features_added[:5])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature engineering impact\n",
    "if 'price_log' in df_eng.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Original price\n",
    "    axes[0, 0].hist(df_eng['price'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Original Price (Right-Skewed)')\n",
    "    axes[0, 0].set_xlabel('Price ($)')\n",
    "    \n",
    "    # Log-transformed price\n",
    "    axes[0, 1].hist(df_eng['price_log'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('Log-Price (More Normal)')\n",
    "    axes[0, 1].set_xlabel('Log(Price)')\n",
    "    \n",
    "    # Q-Q plots\n",
    "    from scipy import stats\n",
    "    stats.probplot(df_eng['price'].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q: Original Price')\n",
    "    \n",
    "    stats.probplot(df_eng['price_log'].dropna(), dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q: Log-Price (Better Fit)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Skewness comparison\n",
    "    from scipy.stats import skew\n",
    "    print(f\"\\nSkewness Improvement:\")\n",
    "    print(f\"  Original Price: {skew(df_eng['price'].dropna()):.3f}\")\n",
    "    print(f\"  Log-Price:      {skew(df_eng['price_log'].dropna()):.3f}\")\n",
    "    print(f\"  (Closer to 0 is better for linear models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STAGE 4: Scale Features\n",
    "\n",
    "### Why Scale?\n",
    "- **Fairness**: Features on different scales (price ~$500K, grade ~10) shouldn't have disproportionate influence\n",
    "- **Convergence**: Gradient-based algorithms converge faster with scaled data\n",
    "- **Distance**: KNN/K-Means use distances (need same scale)\n",
    "- **Interpretation**: Standardized coefficients are comparable\n",
    "\n",
    "### StandardScaler Formula:\n",
    "```\n",
    "scaled_value = (x - mean) / std\n",
    "Result: mean=0, std=1\n",
    "```\n",
    "\n",
    "\n",
    "### Python Code Reference:\n",
    "```python\n",
    "def scale_features(self):\n",
    "    numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
    "    exclude_cols = ['id', 'price']  # Don't scale target/ID\n",
    "    scaler = StandardScaler()\n",
    "    self.data[scale_cols] = scaler.fit_transform(self.data[scale_cols])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df_eng.copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"[4/5] FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = ['id', 'price']  # Don't scale ID or target variable\n",
    "scale_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nScaling {len(scale_cols)} numeric features...\")\n",
    "print(f\"Excluded: {exclude_cols} (ID and target variable)\")\n",
    "\n",
    "# Store original values for comparison\n",
    "original_sqft = df_scaled['sqft_living'].copy() if 'sqft_living' in df_scaled.columns else None\n",
    "original_beds = df_scaled['bedrooms'].copy() if 'bedrooms' in df_scaled.columns else None\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled[scale_cols] = scaler.fit_transform(df_scaled[scale_cols])\n",
    "\n",
    "print(\"\\n Scaling Complete\")\n",
    "print(f\"   Features scaled: {len(scale_cols)}\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nScaling Verification (after scaling):\")\n",
    "print(f\"  sqft_living  â†’ Mean: {df_scaled['sqft_living'].mean():.4f}, Std: {df_scaled['sqft_living'].std():.4f}\")\n",
    "print(f\"  bedrooms     â†’ Mean: {df_scaled['bedrooms'].mean():.4f}, Std: {df_scaled['bedrooms'].std():.4f}\")\n",
    "print(f\"  grade        â†’ Mean: {df_scaled['grade'].mean():.4f}, Std: {df_scaled['grade'].std():.4f}\")\n",
    "\n",
    "# Save scaler for future use\n",
    "scaler_path = '../models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\n Scaler saved to: {scaler_path}\")\n",
    "print(f\"   (Use this to scale test data with same transformation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling impact\n",
    "if original_sqft is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Before scaling\n",
    "    axes[0, 0].hist(original_sqft, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Before Scaling: sqft_living')\n",
    "    axes[0, 0].set_xlabel('Square Feet')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # After scaling\n",
    "    axes[0, 1].hist(df_scaled['sqft_living'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('After Scaling: sqft_living')\n",
    "    axes[0, 1].set_xlabel('Standardized Value')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Before scaling (bedrooms)\n",
    "    axes[1, 0].hist(original_beds, bins=20, color='red', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Before Scaling: bedrooms')\n",
    "    axes[1, 0].set_xlabel('Number of Bedrooms')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # After scaling (bedrooms)\n",
    "    axes[1, 1].hist(df_scaled['bedrooms'], bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_title('After Scaling: bedrooms')\n",
    "    axes[1, 1].set_xlabel('Standardized Value')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n Features now on same scale: meanâ‰ˆ0, stdâ‰ˆ1\")\n",
    "    print(\" Features with vastly different ranges (500-4000 vs 1-10) are now comparable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STAGE 5: Save Data\n",
    "\n",
    "### What We're Saving:\n",
    "1. **Processed Data CSV** - Clean, engineered, scaled features\n",
    "2. **Scaler Pickle** - StandardScaler object for future test data\n",
    "\n",
    "### Why Two Outputs?\n",
    "- **Data CSV**: For analysis, modeling, reporting\n",
    "- **Scaler**: To apply IDENTICAL scaling to test data (critical for model fairness)\n",
    "\n",
    "### Python Code Reference:\n",
    "```python\n",
    "def save_data(self):\n",
    "    self.data.to_csv(self.output_file, index=False)\n",
    "    scaler_file = self.output_file.replace('.csv', '_scaler.pkl')\n",
    "    joblib.dump(self.scaler, scaler_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"[5/5] SAVING PROCESSED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save processed data\n",
    "output_path = '../data/processed/train_cleaned_scaled.csv'\n",
    "df_scaled.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n Data Saved:\")\n",
    "print(f\"   File: {output_path}\")\n",
    "print(f\"   Shape: {df_scaled.shape[0]:,} rows Ã— {df_scaled.shape[1]} columns\")\n",
    "print(f\"   Size: {df_scaled.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n Scaler Saved:\")\n",
    "print(f\"   File: {scaler_path}\")\n",
    "print(f\"   Usage: Apply to test data with joblib.load('{scaler_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Pipeline Summary\n",
    "\n",
    "### Data Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Transformation:\")\n",
    "print(f\"   Raw Data        â†’ {original_count:,} rows Ã— {len(df.columns)} cols\")\n",
    "print(f\"   After Cleaning  â†’ {len(df_clean):,} rows Ã— {len(df_clean.columns)} cols\")\n",
    "print(f\"   After Engineering â†’ {len(df_eng):,} rows Ã— {len(df_eng.columns)} cols (+ {len(features_added)} features)\")\n",
    "print(f\"   After Scaling   â†’ {len(df_scaled):,} rows Ã— {len(df_scaled.columns)} cols (standardized)\")\n",
    "\n",
    "print(f\"\\n Key Metrics:\")\n",
    "print(f\"   Data Retention: {(len(df_scaled)/original_count)*100:.1f}%\")\n",
    "print(f\"   Features Added: {len(features_added)}\")\n",
    "print(f\"   Final Feature Count: {len(df_scaled.columns)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Output Files:\")\n",
    "print(f\"   1. {output_path}\")\n",
    "print(f\"   2. {scaler_path}\")\n",
    "\n",
    "print(f\"\\n Next Steps:\")\n",
    "print(f\"   1. Load processed data: df = pd.read_csv('{output_path}')\")\n",
    "print(f\"   2. Split into train/test (e.g., 80/20)\")\n",
    "print(f\"   3. Train baseline model (without engineered features)\")\n",
    "print(f\"   4. Train enhanced model (with engineered features)\")\n",
    "print(f\"   5. Compare performance (RMSE, RÂ², MAE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using the Reusable Pipeline Class\n",
    "\n",
    "For production use, you can use the `SimpleDataPipeline` class from the Python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the production pipeline class\n",
    "\n",
    "class SimpleDataPipeline:\n",
    "    \"\"\"All-in-one data preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, input_file, output_file=\"processed_data.csv\"):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.data = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute complete pipeline\"\"\"\n",
    "        self.load_data()\n",
    "        self.clean_data()\n",
    "        self.engineer_features()\n",
    "        self.scale_features()\n",
    "        self.save_data()\n",
    "        \n",
    "        print(f\" Pipeline Complete!\")\n",
    "        print(f\"   Output: {self.output_file}\")\n",
    "        print(f\"   Shape: {self.data.shape}\")\n",
    "        return self.data\n",
    "    \n",
    "    # [load_data, clean_data, engineer_features, scale_features, save_data methods...]\n",
    "\n",
    "# Usage:\n",
    "# pipeline = SimpleDataPipeline('../data/raw/train.xlsx', '../data/processed/train_processed.csv')\n",
    "# processed_df = pipeline.run()\n",
    "\n",
    "print(\"âœ“ Production pipeline class defined\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  pipeline = SimpleDataPipeline('input.xlsx', 'output.csv')\")\n",
    "print(\"  df = pipeline.run()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### ðŸŽ¯ Quality Checks:\n",
    "- [ ] No missing values after cleaning?\n",
    "- [ ] Outlier removal reasonable? (< 10% loss)\n",
    "- [ ] Features scaled to meanâ‰ˆ0, stdâ‰ˆ1?\n",
    "- [ ] Log transforms improve skewness?\n",
    "- [ ] Scaler saved for test data?\n",
    "- [ ] Feature engineering increased predictive power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£  DATA INTEGRITY\")\n",
    "print(f\"   Missing values: {df_scaled.isnull().sum().sum()} (âœ“ clean)\")\n",
    "print(f\"   Duplicate rows: {df_scaled.duplicated().sum()} (âœ“ none)\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£  FEATURE STATISTICS\")\n",
    "for col in df_scaled.select_dtypes(include=[np.number]).columns[:5]:\n",
    "    print(f\"   {col}: Î¼={df_scaled[col].mean():.4f}, Ïƒ={df_scaled[col].std():.4f}, min={df_scaled[col].min():.4f}, max={df_scaled[col].max():.4f}\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£  DATA RETENTION\")\n",
    "print(f\"   Original â†’ Final: {original_count:,} â†’ {len(df_scaled):,} ({(len(df_scaled)/original_count)*100:.1f}%)\")\n",
    "print(f\"   Outliers removed: {original_count - len(df_cleaned):,}\")\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£  FEATURE ENGINEERING\")\n",
    "print(f\"   Features created: {len(features_added)}\")\n",
    "print(f\"   Original columns: {len(df.columns)}\")\n",
    "print(f\"   Final columns: {len(df_scaled.columns)}\")\n",
    "\n",
    "print(f\"\\n5ï¸âƒ£  MODEL READINESS\")\n",
    "print(f\"   âœ“ All features scaled (meanâ‰ˆ0, stdâ‰ˆ1)\")\n",
    "print(f\"   âœ“ Log transforms applied (reduced skewness)\")\n",
    "print(f\"   âœ“ Domain features engineered\")\n",
    "print(f\"   âœ“ Data saved to: {output_path}\")\n",
    "print(f\"   âœ“ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\" PREPROCESSING COMPLETE - READY FOR MODELING\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
